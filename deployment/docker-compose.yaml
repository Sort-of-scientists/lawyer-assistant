services:
  llamacpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server
    ports:
      - 8080:8080
    volumes:
      - ./backend/models:/models
    command: --lora /models/Cotype-Nano-Generate-Adapter-q8_0.gguf --lora /models/Cotype-Nano-Summary-Adapter-q8_0.gguf
    environment:
      LLAMA_ARG_MODEL: /models/Cotype-Nano.Q8_0.gguf
      LLAMA_ARG_CTX_SIZE: 2048
      LLAMA_ARG_PORT: 8080
    networks:
      - app-network    
  frontend:
      container_name: frontend
      build:
        context: ./frontend
        dockerfile: Dockerfile
      environment:
        - VITE_API_URL=http://127.0.0.1:8018/api
      networks:
        - app-network

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
      
    environment:
      - MONGO_URI=mongodb://mongodb:27017/
      - MONGO_DATABASE=database
      - MONGO_COLLECTION=documents

      - SUMMARY_LORA_ADAPTER_ID=1
      - GENERATE_LORA_ADAPTER_ID=0
    depends_on:
      - tika
      - mongodb
    networks:
      - app-network
  nginx:
    image: nginx:alpine
    ports:
      - "8018:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - frontend
      - backend
    networks:
      - app-network

  mongodb:
    image: mongo
    ports:
      - "8001:27017"
    volumes:
      - mongodb_data:/data/db
    networks:
      - app-network

  tika:
    image: apache/tika

volumes:
  mongodb_data:

networks:
  app-network:
    driver: bridge
